import gradio as gr
import edge_tts
import asyncio
import tempfile
import os
from pydub import AudioSegment
import re
import datetime
import pytz # á€¡á€á€»á€­á€”á€ºá€€á€­á€¯ á€™á€¼á€”á€ºá€™á€¬á€…á€¶á€á€±á€¬á€ºá€á€»á€­á€”á€ºá€”á€²á€· á€•á€¼á€–á€­á€¯á€·

# --- Global Variables ---
SESSION_COUNT = 0
LAST_USED_TIME = "á€™á€›á€¾á€­á€á€±á€¸á€•á€«"

# --- Setup ---
VOICES = [
    ("á€¡á€€á€­á€¯á€œá€±á€¸ (Male)", "my-MM-ThihaNeural"),
    ("á€™á€¼á€™á€¼ (Female)", "my-MM-NilarNeural"),
    ("English UK (Female)", "en-GB-SoniaNeural"),
    ("English UK (Male)", "en-GB-RyanNeural")
]

DEFAULT_RULES = """
á€™á€±á€á€¹á€á€¬ = á€™á€¼á€…á€ºá€á€¬
á€á€…á€¹á€…á€¬ = á€á€…á€ºá€…á€¬
á€•á€¼á€¿á€”á€¬ = á€•á€¼á€á€ºá€á€”á€¬
á€¥á€…á€¹á€…á€¬ = á€¡á€¯á€á€ºá€…á€¬
á€¥á€®á€¸á€”á€¾á€±á€¬á€€á€º = á€¡á€¯á€¶á€¸á€”á€¾á€±á€¬á€€á€º
"""

# --- Helper Functions ---
def remove_silence_from_end(audio_chunk, silence_thresh=-40.0, chunk_size=10):
    try:
        reversed_audio = audio_chunk.reverse()
        trim_ms = 0
        while trim_ms < len(reversed_audio):
            chunk = reversed_audio[trim_ms:trim_ms+chunk_size]
            if chunk.dBFS > silence_thresh:
                break
            trim_ms += chunk_size
        return reversed_audio[trim_ms:].reverse() if trim_ms < len(reversed_audio) else audio_chunk
    except Exception as e:
        print(f"Silence removal error: {e}")
        return audio_chunk

def format_srt_time(seconds):
    millis = int((seconds - int(seconds)) * 1000)
    seconds = int(seconds)
    minutes = seconds // 60
    hours = minutes // 60
    minutes %= 60
    seconds %= 60
    return f"{hours:02}:{minutes:02}:{seconds:02},{millis:03}"

def apply_pronunciation_rules(text, rules_str):
    if not rules_str: return text
    for line in rules_str.split('\n'):
        if '=' in line:
            parts = line.split('=')
            if len(parts) >= 2:
                text = text.replace(parts[0].strip(), parts[1].strip())
    return text

def split_by_punctuation(text):
    raw_segments = re.split(r'([!á‹.;!?áŠ])', text)
    refined_segments = []
    current_text = ""
    for seg in raw_segments:
        if not seg: continue
        if seg in ['á‹', '.', '!', ';', '?', 'áŠ']:
            if current_text:
                current_text += seg
                refined_segments.append(current_text.strip())
                current_text = ""
            elif refined_segments:
                refined_segments[-1] += seg
        else:
            current_text += seg
    if current_text.strip(): refined_segments.append(current_text.strip())
    return refined_segments

def smart_split_text_for_srt(text, max_chars):
    words = text.split(' ')
    segments = []
    current = ""
    for word in words:
        if len(current) + len(word) < max_chars:
            current += word + " "
        else:
            if current.strip(): segments.append(current.strip())
            current = word + " "
    if current.strip(): segments.append(current.strip())
    return segments

async def generate_precise_audio(text, pronunciation_rules, voice_key, rate_str, pitch_str, volume_boost, max_srt_chars):
    combined_audio = AudioSegment.empty()
    srt_content = ""
    current_audio_time = 0.0
    subtitle_index = 1
    
    text = text.replace("\n", " ")
    audio_segments = split_by_punctuation(text)
    
    temp_dir = tempfile.mkdtemp()
    
    for i, audio_text_segment in enumerate(audio_segments):
        if not audio_text_segment.strip(): continue
        
        speakable_text = apply_pronunciation_rules(audio_text_segment.strip(), pronunciation_rules)
        temp_fname = os.path.join(temp_dir, f"part_{i}.mp3")
        
        try:
            communicate = edge_tts.Communicate(speakable_text, voice_key, rate=rate_str, pitch=pitch_str)
            await communicate.save(temp_fname)
            
            audio_segment = AudioSegment.from_mp3(temp_fname)
            audio_segment = remove_silence_from_end(audio_segment)
            
            segment_duration = len(audio_segment) / 1000.0
            combined_audio += audio_segment
            combined_audio += AudioSegment.silent(duration=50)

            srt_chunks = smart_split_text_for_srt(audio_text_segment, max_chars=max_srt_chars)
            total_chars = len(audio_text_segment) or 1
            segment_start_time = current_audio_time
            
            for chunk in srt_chunks:
                chunk_len = len(chunk)
                chunk_duration = (chunk_len / total_chars) * segment_duration
                if chunk_duration < 0.5 and len(srt_chunks) > 1: chunk_duration = 0.5 

                start_srt = format_srt_time(segment_start_time)
                end_srt = format_srt_time(segment_start_time + chunk_duration)
                srt_content += f"{subtitle_index}\n{start_srt} --> {end_srt}\n{chunk.strip()}\n\n"
                
                subtitle_index += 1
                segment_start_time += chunk_duration

            current_audio_time += segment_duration + 0.05
            
        except Exception as e:
            print(f"Error: {e}")
            continue

    if volume_boost > 0:
        combined_audio = combined_audio + volume_boost
            
    return combined_audio, srt_content

async def generate_audio_final(text, rules, voice_name, tone_val, speed_val, volume_val, filename_val, platform_val):
    if not text.strip(): raise gr.Error("á€…á€¬á€›á€­á€¯á€€á€ºá€‘á€Šá€·á€ºá€•á€«!")

    # --- COUNTER & TIME LOGIC ---
    global SESSION_COUNT, LAST_USED_TIME
    
    SESSION_COUNT += 1
    
    # á€™á€¼á€”á€ºá€™á€¬á€…á€¶á€á€±á€¬á€ºá€á€»á€­á€”á€º (MMT)
    tz_MM = pytz.timezone('Asia/Yangon')
    datetime_MM = datetime.datetime.now(tz_MM)
    LAST_USED_TIME = datetime_MM.strftime("%I:%M %p")

    # Status Message
    status_msg = f"ğŸ“Š á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€™á€¾á€¯: {SESSION_COUNT} á€€á€¼á€­á€™á€º | ğŸ•’ á€”á€±á€¬á€€á€ºá€†á€¯á€¶á€¸á€á€¯á€¶á€¸á€á€»á€­á€”á€º: {LAST_USED_TIME}"
    print(f"\nğŸ”” [USAGE] Count: {SESSION_COUNT} | Time: {LAST_USED_TIME}\n")
    # ----------------------------
    
    target_key = "my-MM-ThihaNeural"
    if "Female" in str(voice_name): target_key = "my-MM-NilarNeural"
    elif "Ryan" in str(voice_name): target_key = "en-GB-RyanNeural"
    elif "Sonia" in str(voice_name): target_key = "en-GB-SoniaNeural"
            
    pitch_str = f"{int(tone_val) * -1:+d}Hz"
    rate_str = f"{int(speed_val):+d}%"
    srt_limit = 150 if "YouTube" in platform_val else 55

    try:
        final_audio, srt_text = await generate_precise_audio(text, rules, target_key, rate_str, pitch_str, volume_val, srt_limit)
        
        output_dir = tempfile.mkdtemp()
        base_name = "output"
        if filename_val and filename_val.strip():
            base_name = "".join(c for c in filename_val if c.isalnum() or c in (' ', '-', '_')).strip()
        
        audio_path = os.path.join(output_dir, f"{base_name}.mp3")
        final_audio.export(audio_path, format="mp3")
        
        srt_path = os.path.join(output_dir, f"{base_name}.srt")
        with open(srt_path, "w", encoding="utf-8") as f:
            f.write(srt_text)

        return audio_path, srt_path, status_msg
        
    except Exception as e:
        raise gr.Error(f"Error: {str(e)}")

with gr.Blocks(title="Myanmar TTS Pro") as demo:
    gr.Markdown("## á€™á€¼á€”á€ºá€™á€¬ TTS Pro (Render HQ)")
    with gr.Row():
        with gr.Column():
            voice = gr.Dropdown([v[0] for v in VOICES], value="á€¡á€€á€­á€¯á€œá€±á€¸ (Male)", label="Voice")
            platform = gr.Radio(["TikTok (9:16)", "YouTube (16:9)"], value="TikTok (9:16)", label="SRT Type")
            
            # Updated Settings: Pitch=7, Speed=25, Vol=10
            tone = gr.Slider(-50, 50, value=7, label="Pitch")
            speed = gr.Slider(-50, 50, value=25, label="Speed")
            vol = gr.Slider(0, 20, value=10, label="Vol Boost")
            
            text = gr.Textbox(lines=5, label="Text")
            rules = gr.Textbox(lines=5, value=DEFAULT_RULES, label="Rules")
            fname = gr.Textbox(label="File Name")
            
            btn = gr.Button("Generate", variant="primary")
            lbl_status = gr.Label(value="ğŸ“Š á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€™á€¾á€¯: 0 | ğŸ•’ á€…á€±á€¬á€„á€·á€ºá€†á€­á€¯á€„á€ºá€¸á€”á€±á€•á€«á€á€Šá€º...", label="Live Status")
            
        with gr.Column():
            out_aud = gr.Audio(label="Audio")
            out_srt = gr.File(label="SRT")
            
    btn.click(generate_audio_final, inputs=[text, rules, voice, tone, speed, vol, fname, platform], outputs=[out_aud, out_srt, lbl_status])

if __name__ == "__main__":
    demo.queue(concurrency_count=2).launch(server_name="0.0.0.0", server_port=7860)
